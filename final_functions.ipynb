{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qYwn-GWPBqzO"
   },
   "source": [
    "**Objective** : To provide sentence completion suggestions.\n",
    "\n",
    "**Raw data/Input** : input sentence from user (partial sentence) that needs to be completed and sentences before that.\n",
    "\n",
    "**Output/predictions/suggestions** : suggestions/partial sentences that completes the input sentence and make sense.\n",
    "\n",
    "**loss metric** : sparse categorical crossentropy and perplexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "gT_L7WHAGpCA"
   },
   "outputs": [],
   "source": [
    "#required libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import  Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "XiMq8qJ3Gx7F"
   },
   "outputs": [],
   "source": [
    "#required files\n",
    "infile = open('embedding_matrix','rb')\n",
    "embeddings_index = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "infile = open('bigru_model','rb')\n",
    "bigru_model = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "infile = open('tokenizer','rb')\n",
    "tokenizer = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "KvLghFUKEP8x"
   },
   "outputs": [],
   "source": [
    "#loss functions\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def sparsecce(real, pred):\n",
    "    \"\"\"gets perplexity values accounting for masked values too\"\"\"\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "kBZkJyFLD1-X"
   },
   "outputs": [],
   "source": [
    "#required basic dictionaries\n",
    "\n",
    "index_word={}\n",
    "for key,value in tokenizer.word_index.items():\n",
    "  index_word[value]=key \n",
    "\n",
    "CONTRACTION_MAP = {\n",
    "\"ain't\": \"is not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "c24CVX_51B5i"
   },
   "outputs": [],
   "source": [
    "#SAMPLE input and real output\n",
    "#Here we have take a sample mail and clip the final sentence partially to get suggestions\n",
    "sample_input =\"\"\"\n",
    "Jacques,\n",
    "\n",
    "Did you receive the fax from Kevin Kolb with a small change on page 2. \n",
    "It also included the conveyance and assignment document. \n",
    "Does this look ok to you? Is the language \"Transfer will occur when first lien holder, Pacific Southwest Bank, approves the assignment\" sufficient? \n",
    "Hopefully it is ok. Let me know if it is\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "wanted_suggestion = \"\"\"and i will come by\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "2aMuodZN7Csu",
    "outputId": "b8b8be1c-87c4-45fa-a7a0-652498dee369"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\nJacques,\\n\\nDid you receive the fax from Kevin Kolb with a small change on page 2. \\nIt also included the conveyance and assignment document. \\nDoes this look ok to you? Is the language \"Transfer will occur when first lien holder, Pacific Southwest Bank, approves the assignment\" sufficient? \\nHopefully it is ok. Let me know if it is\\n\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "Z0sk8DVi6s0T",
    "outputId": "9ba1fc65-14e8-4dc0-f27b-882284bae849"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'and i will come by'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wanted_suggestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "mA2zb2utTTR-"
   },
   "outputs": [],
   "source": [
    "#function to return suggestions\n",
    "\n",
    "def final1(input_string,beam_search=False,k=3,max_len=12,no_suggestions=3):\n",
    "\n",
    "  \"\"\"takes input string and gives sentence completion suggestions\"\"\"\n",
    "\n",
    "  #Preprocessing steps\n",
    "  textdata = input_string\n",
    "  textdata = re.sub(r\"[a-zA-Z0-9_+.-]+@+[a-zA-Z0-9_+.-]+\",\" \",textdata) #removes mail ids\n",
    "  textdata = re.sub(r'http\\S+|www\\S+',\" \", textdata) #removes links/URLs\n",
    "  textdata = re.sub(r\"<.*>\",\" \",textdata) #removes string enclosed within < >\n",
    "  textdata = re.sub(r\"\\[.*\\]\",\" \",textdata) #removes string enclosed within [ ]\n",
    "  textdata = re.sub(r\"\\(.*\\)\",\" \",textdata) #removes string enclosed within ( )\n",
    "  textdata = re.sub(r\"[\\n]{2,}\",\".\",textdata) #replaces double or more new line char with '.'\n",
    "  textdata = re.sub(r\"[\\t\\n]\",\" \",textdata) #removes tabs, single new line char\n",
    "  textdata = re.sub(r\"[\\-\\\\\\_\\/]\",\" \",textdata) #removes '-' '\\' '_' and '/'\n",
    "  textdata = re.sub(r\"[a-zA-Z]+:\",\" \",textdata) #removes words that end with ':'\n",
    "  textdata = re.sub(r\"\\b_|_\\b\",\" \",textdata) #removes \"_\" at start\\end of words\n",
    "  textdata = re.sub(r'AM|PM|A\\.M|P\\.M|a\\.m|p\\.m|pm',\" \",textdata) #remove timestamp texts \n",
    "  textdata = re.sub(r\"\\d+:\\d+\\.\",\".\",textdata) #removes time, if at end and retains sentence\n",
    "  textdata = textdata.lower() #makes all text lowercase\n",
    "  x = re.findall(r'\\S+\\'\\S+',textdata)\n",
    "  if x is not None:\n",
    "    for word in x:\n",
    "      if word in CONTRACTION_MAP: #decontracts words\n",
    "        textdata = textdata.replace(word, CONTRACTION_MAP[word.lower()])\n",
    "  textdata = re.sub(r'[^a-zA-z!?,. ]',' ', textdata) #remove everything except [^a-zA-z!?,. ]\n",
    "  textdata = re.sub(r'[?!,]','.', textdata) #changes '?!,' to '.'\n",
    "  textdata = re.sub(r' +', ' ', textdata) #removes extra spaces in between string\n",
    "  textdata = re.sub(r' *\\.','.', textdata) #removing blank sentences\n",
    "  textdata = re.sub(r'\\.+', '.', textdata) #convert multidots to single\n",
    "  textdata = re.sub(r'\\. ','.',textdata) #removing space at start of each sentence\n",
    "  textdata = textdata.strip('. ') #removes '.' and spaces in start\\end of strings\n",
    "\n",
    "  textdata = textdata.split('.')\n",
    "  textdata = textdata[-1]    #only last sentence that need suggestions is retained \n",
    "  if len(textdata.split()) >= 22:   #if last sentence has more than 21 tokens , last 21 are retained\n",
    "    textdata = textdata.split()[-21:] \n",
    "\n",
    " #tokenizing steps\n",
    "  textdata = \"<start> \" + textdata + \" <end>\" #adding start and end tokens\n",
    "  data_sequence = tokenizer.texts_to_sequences([textdata,textdata])   #tokenizer takes minimum two elements\n",
    "  encoder_seq = pad_sequences(data_sequence, maxlen=23, padding='post')\n",
    "  encoder_seq = tf.expand_dims(encoder_seq[0], axis=0)\n",
    "  #encoding part\n",
    "  encoder_out, fw_state_h, ba_state_h = bigru_model.layers[3](bigru_model.layers[1](bigru_model.layers[0](encoder_seq)))\n",
    "\n",
    "  #greedy search steps\n",
    "  if beam_search == False:\n",
    "    state_h = Concatenate()([fw_state_h, ba_state_h])\n",
    "    dec_input = np.zeros((1,1))\n",
    "    dec_input[0,0] = tokenizer.word_index['<start>']\n",
    "    stop_condition=False\n",
    "    sent=''\n",
    "    \n",
    "    #decoding part\n",
    "    while not stop_condition:\n",
    "        predicted_out, state_h = bigru_model.layers[6](bigru_model.layers[4](bigru_model.layers[2](dec_input)),initial_state=state_h)\n",
    "        dense_out = bigru_model.layers[7](predicted_out)\n",
    "        output = np.argmax(dense_out)                    #argmax ~= greedy search\n",
    "        dec_input = np.reshape(output, (1, 1))\n",
    "        if index_word[output] == '<end>':\n",
    "            stop_condition=True\n",
    "        else:\n",
    "            sent=sent + ' ' + index_word[output]\n",
    "\n",
    "    return sent\n",
    "\n",
    "  #beam_search steps\n",
    "  else:\n",
    "    states = Concatenate()([fw_state_h, ba_state_h])\n",
    "\n",
    "    #variable declaration\n",
    "    start_token = tokenizer.word_index['<start>']\n",
    "    end_token   = tokenizer.word_index['<end>']\n",
    "    eos_sent = []\n",
    "    eosent_score = []\n",
    "    top_ksent = [[start_token]]\n",
    "    top_kscore = [0]\n",
    "    counter = 0\n",
    "\n",
    "    #beam_search loop\n",
    "    while counter<max_len: \n",
    "      temp_sent  = []\n",
    "      temp_score = []\n",
    "      counter += 1\n",
    "      for i,sent in enumerate(top_ksent):\n",
    "        state_h = states\n",
    "        for j in range(len(sent)):\n",
    "          dec_input = np.reshape(sent[j],(1,1))\n",
    "          predicted_out, state_h = bigru_model.layers[6](bigru_model.layers[4](bigru_model.layers[2](dec_input)),initial_state=state_h)      \n",
    "        score_0 = bigru_model.layers[7](predicted_out)[0,0,:]\n",
    "        top_k0 = (np.argsort(score_0)[::-1][:k]).tolist()\n",
    "        k_scores = np.log((np.sort(score_0)[::-1][:k])).tolist()\n",
    "        if end_token in top_k0:\n",
    "          eos_sent.append([*sent,end_token])\n",
    "          eosent_score.append((top_kscore[i]+k_scores[top_k0.index(end_token)])/2)\n",
    "          del k_scores[top_k0.index(end_token)]\n",
    "          del top_k0[top_k0.index(end_token)]\n",
    "        temp_sent.extend([[*sent,m] for m in top_k0])\n",
    "        temp_score.extend([(m+top_kscore[i])/2 for m in k_scores])\n",
    "    \n",
    "      top_ksent =  [temp_sent[l] for l in (np.argsort(temp_score)[::-1][:k]).tolist()]\n",
    "      top_kscore = (np.sort(temp_score)[::-1][:k]).tolist()\n",
    "\n",
    "    #tokens to sentence conversion\n",
    "    eos_index = (np.argsort(eosent_score)[::-1][:no_suggestions]).tolist()\n",
    "    eos_sent = [eos_sent[h] for h in eos_index]\n",
    "    eosent_score = (np.sort(eosent_score)[::-1][:no_suggestions]).tolist()\n",
    "    \n",
    "    best_sentences = []\n",
    "    for p,sentence in enumerate(eos_sent):\n",
    "      best_sent = ''\n",
    "      for word in sentence[1:-1]:\n",
    "        best_sent += ' ' + index_word[word]\n",
    "      best_sentences.append(best_sent)\n",
    "    \n",
    "    return best_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "L-YALnIj5BAo",
    "outputId": "6f3084ce-0b1d-4a84-f1e5-7d306583c769"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 150 ms, sys: 2.83 ms, total: 153 ms\n",
      "Wall time: 154 ms\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' not too much trouble'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getting suggestions using sample and final1 with greedy search\n",
    "%%time\n",
    "suggestion = final1(sample_input)\n",
    "suggestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IPNHWcko5gG8",
    "outputId": "492d61e7-5bf3-49d8-93d1-401d7432fc3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.76 s, sys: 15.8 ms, total: 1.78 s\n",
      "Wall time: 1.78 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' a good time for you',\n",
       " ' not too much trouble',\n",
       " ' a good time for me to stop by']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getting suggestions using sample and final1 with beam search\n",
    "%%time\n",
    "top_suggestions = final1(sample_input,beam_search=True)\n",
    "top_suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "MhzleP3kiNiQ"
   },
   "outputs": [],
   "source": [
    "#function to return loss values\n",
    "\n",
    "def final2(input_string,output_string):\n",
    "\n",
    "  \"\"\"takes input,output string and gives loss values\"\"\"\n",
    "\n",
    "  #Preprocessing steps\n",
    "  textdata = input_string\n",
    "  textdata = re.sub(r\"[a-zA-Z0-9_+.-]+@+[a-zA-Z0-9_+.-]+\",\" \",textdata) #removes mail ids\n",
    "  textdata = re.sub(r'http\\S+|www\\S+',\" \", textdata) #removes links/URLs\n",
    "  textdata = re.sub(r\"<.*>\",\" \",textdata) #removes string enclosed within < >\n",
    "  textdata = re.sub(r\"\\[.*\\]\",\" \",textdata) #removes string enclosed within [ ]\n",
    "  textdata = re.sub(r\"\\(.*\\)\",\" \",textdata) #removes string enclosed within ( )\n",
    "  textdata = re.sub(r\"[\\n]{2,}\",\".\",textdata) #replaces double or more new line char with '.'\n",
    "  textdata = re.sub(r\"[\\t\\n]\",\" \",textdata) #removes tabs, single new line char\n",
    "  textdata = re.sub(r\"[\\-\\\\\\_\\/]\",\" \",textdata) #removes '-' '\\' '_' and '/'\n",
    "  textdata = re.sub(r\"[a-zA-Z]+:\",\" \",textdata) #removes words that end with ':'\n",
    "  textdata = re.sub(r\"\\b_|_\\b\",\" \",textdata) #removes \"_\" at start\\end of words\n",
    "  textdata = re.sub(r'AM|PM|A\\.M|P\\.M|a\\.m|p\\.m|pm',\" \",textdata) #remove timestamp texts \n",
    "  textdata = re.sub(r\"\\d+:\\d+\\.\",\".\",textdata) #removes time, if at end and retains sentence\n",
    "  textdata = textdata.lower() #makes all text lowercase\n",
    "  x = re.findall(r'\\S+\\'\\S+',textdata)\n",
    "  if x is not None:\n",
    "    for word in x:\n",
    "      if word in CONTRACTION_MAP: #decontracts words\n",
    "        textdata = textdata.replace(word, CONTRACTION_MAP[word.lower()])\n",
    "  textdata = re.sub(r'[^a-zA-z!?,. ]',' ', textdata) #remove everything except [^a-zA-z!?,. ]\n",
    "  textdata = re.sub(r'[?!,]','.', textdata) #changes '?!,' to '.'\n",
    "  textdata = re.sub(r' +', ' ', textdata) #removes extra spaces in between string\n",
    "  textdata = re.sub(r' *\\.','.', textdata) #removing blank sentences\n",
    "  textdata = re.sub(r'\\.+', '.', textdata) #convert multidots to single\n",
    "  textdata = re.sub(r'\\. ','.',textdata) #removing space at start of each sentence\n",
    "  textdata = textdata.strip('. ') #removes '.' and spaces in start\\end of strings\n",
    "\n",
    "  textdata = textdata.split('.')\n",
    "  textdata = textdata[-1]    #only last sentence that need suggestions is retained \n",
    "  if len(textdata.split()) >= 22:   #if last sentence has more than 21 tokens , last 21 are retained\n",
    "    textdata = textdata.split()[-21:] \n",
    "\n",
    "  #tokenizing steps\n",
    "  encoder_input = \"<start> \" + textdata + \" <end>\" #adding start and end tokens\n",
    "  decoder_input = \"<start> \" + output_string \n",
    "  decoder_output = output_string + \" <end>\"\n",
    "  data_sequence = tokenizer.texts_to_sequences([encoder_input,encoder_input]) #tokenizer takes min 2 inputs\n",
    "  encoder_seq = pad_sequences(data_sequence, maxlen=23, padding='post')\n",
    "  encoder_seq = tf.expand_dims(encoder_seq[0], axis=0)\n",
    "\n",
    "  decoder_input = tokenizer.texts_to_sequences([decoder_input,decoder_input])\n",
    "  decoder_in_seq = pad_sequences(decoder_input, maxlen=22, padding='post')\n",
    "  decoder_in_seq = tf.expand_dims(decoder_in_seq[0], axis=0)\n",
    "\n",
    "  decoder_output = tokenizer.texts_to_sequences([decoder_output,decoder_output])\n",
    "  decoder_out_seq = pad_sequences(decoder_output, maxlen=22, padding='post')\n",
    "  decoder_out_seq = tf.expand_dims(decoder_out_seq[0], axis=0)\n",
    "\n",
    "  #getting predictions and loss\n",
    "  probs = bigru_model.predict([encoder_seq,decoder_in_seq])\n",
    "  sparse_cce = sparsecce(decoder_out_seq,probs)\n",
    "  perplexity = tf.exp(sparse_cce)\n",
    "  return sparse_cce,perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1xBhdMWW5vPx",
    "outputId": "1e4a1f87-3595-4983-8718-d848cc243524"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.29 s, sys: 97.8 ms, total: 5.39 s\n",
      "Wall time: 6.42 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=1.8820548>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=6.5669847>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getting loss values (sparcecce and perplexity) using sample input, expected sentence completion using final2\n",
    "%%time\n",
    "loss_values = final2(sample_input,wanted_suggestion)\n",
    "loss_values"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
